{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with we need a game design and a map on which to play it. We start off with something very simple; a room with a goal G in one corner and the agent A in the other, with some walls w between and around the outside, and a hole H that the agent must not fall into. In particular we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pygame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ea23980772e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pyvgdlmaster/vgdl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmdpmap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMDPconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGDLParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# from interfaces import GameEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lewishammond/Desktop/werk/pv4dsrl/pyvgdlmaster/vgdl/mdpmap.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpybrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0montology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBASEDIRS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minterfaces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGameEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lewishammond/Desktop/werk/pv4dsrl/pyvgdlmaster/vgdl/ontology.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtriPoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munitVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectNorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moncePerStep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAStarWorld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pygame"
     ]
    }
   ],
   "source": [
    "# Define level\n",
    "level = \"\"\"\n",
    "wwwwwwwwwwwww\n",
    "wA       w  w\n",
    "w   w      ww\n",
    "www w   wwwww\n",
    "w       wwG w\n",
    "w H  w      w\n",
    "wwwwwwwwwwwww\n",
    "\"\"\"\n",
    "\n",
    "# Define game\n",
    "game = \"\"\"\n",
    "BasicGame \n",
    "    LevelMapping\n",
    "        G > goal\n",
    "        A > avatar\n",
    "        H > hole\n",
    "        w > wall\n",
    "        \n",
    "    InteractionSet\n",
    "        avatar wall > stepBack\n",
    "        goal avatar > killSprite\n",
    "        avatar hole > killSprite\n",
    "\n",
    "    SpriteSet  \n",
    "        structure > Immovable\n",
    "            goal > color=GREEN\n",
    "            hole > color=RED\n",
    "            wall > color=BROWN\n",
    "            \n",
    "    TerminationSet\n",
    "        # SpriteCounter stype=goal limit=0 win=True\n",
    "        SpriteCounter stype=goal   win=True\n",
    "        SpriteCounter stype=avatar win=False\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary functions\n",
    "import sys\n",
    "sys.path.insert(0, 'pyvgdlmaster/vgdl')\n",
    "from mdpmap import MDPconverter\n",
    "from core import VGDLParser\n",
    "from rlenvironment import RLEnvironment\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "# Start game and produce image\n",
    "g = VGDLParser().parseGame(game)\n",
    "g.buildLevel(level)\n",
    "rle = RLEnvironment(game, level, observationType='global', visualize=True)\n",
    "rle._game._drawAll()\n",
    "pygame.image.save(rle._game.screen, \"example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent gets +10 points for reaching the goal, -10 points for falling in the hole, and -1 point every turn that it has not reached either the goal or the hole (so it has an incentive to reach the goal as quickly as possible). This ends up giving us the following game:\n",
    "\n",
    "<img src=\"example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to be able to extract information in the form of (state, reward, action)\\_t for each time step t so as to be able to learn schemas. Each state can be extracted from the VGDL in the form of a list that describes the world in terms of objects and their positions, with a possible action and reward at each state, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up RLE\n",
    "rle.actionDelay = 200\n",
    "rle.recordingEnabled = True\n",
    "rle.reset()\n",
    "\n",
    "# Get intial state information\n",
    "initState = rle._obstypes.copy()\n",
    "state = rle.getState()\n",
    "initState['agent'] = [(state[0], state[1])]\n",
    "initReward = 0\n",
    "\n",
    "# Initialise parameters\n",
    "numSteps = 1\n",
    "actions = np.array([0,0,1,0])\n",
    "ended = False\n",
    "rStates = []\n",
    "rReward = []\n",
    "rActions = []\n",
    "rStates.append(initState)\n",
    "rReward.append(initReward)\n",
    "\n",
    "# Perform sequence of actions\n",
    "for i in range(numSteps):\n",
    "    if ended == False:\n",
    "        # Take and record action\n",
    "        rle._performAction(actions)\n",
    "        action = rle._allEvents[-1][1]\n",
    "        rActions.append(action)\n",
    "        # Get and record new state information\n",
    "        newState = rle._obstypes.copy()\n",
    "        state = rle.getState()\n",
    "        newState['agent'] = [(state[0], state[1])]\n",
    "        rStates.append(newState)\n",
    "        # Get and record new reward information\n",
    "        (ended, won) = rle._isDone()\n",
    "        if ended:\n",
    "            if won:\n",
    "                newReward = 10\n",
    "            else:\n",
    "                newReward = -10\n",
    "        else:\n",
    "            newReward = -1\n",
    "        rRewards.append(newReward)\n",
    "# Record final action as None\n",
    "rActions.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the raw state data above we binary-encode sets of matrices for learning schemas. See https://www.overleaf.com/read/nrfnchwgmpdg for details on the form of the matrices required. The schemas are learnt as columns of weight matrices and then converted to logical representations for use in planning and policy formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
